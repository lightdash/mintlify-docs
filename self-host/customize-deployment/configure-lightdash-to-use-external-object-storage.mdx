---
title: "Configure Lightdash to use external storage"
sidebarTitle: "External storage"
---

On Lightdash, we generate some files like:

- Images for Slack unfurl
- Images on scheduled deliveries
- Results on CSV format.

These files need to be stored in a S3 Compatible Cloud storage. Some options are GCP Buckets, S3 Storage and MinIO.

## Configure cloud storage on Google Cloud Platform

Go into GCP: Cloud storage

Create a new bucket with the following details:

- give it an unique `Bucket name` like - `lightdash-cloud-file-storage-eu`
- Select region (like multi US or multi EU or single region EU-west-1)
- select storage class: default standard
- disable `enforce public access` and select `fine-grained`
- Protection: `none`

Go to settings \> Interoperability and create a `Access key for service account`

- Copy the `access key` and `secret`

`S3_ENDPOINT` for google is `https://storage.googleapis.com` `S3_REGION` for google is `auto`

## Configure cloud storage on AWS

- Navigate to the S3 section of the AWS Management Console and click on the Create Bucket button.
- Give your bucket a name and select the region where you want to store your data.
- Next, you need to set the permissions for your bucket. Make it private.

To export your S3 credentials, you need to follow these steps:

- Navigate to the IAM section and click on the Users tab.
- Click on the user whose credentials you want to export.
- Click on the Security Credentials tab and locate the Access Keys section.
- Click on the Create Access Key button.
- Download the CSV file that contains your Access Key ID and Secret Access Key.

Check [this guide](https://docs.aws.amazon.com/general/latest/gr/s3.html) to see what's the right `S3_ENDPOINT` for your bucket

## Configure cloud storage using MinIO

Creating a bucket in MinIO

- Login to the MinIO console and click on "Buckets" in the side bar
- Click on "Create Bucket"
- Give your bucket a name and click "Create Bucket"

Creating access credentials in MinIO

- Click on "Access Keys" in the side bar
- Click "Create access key"
- Give a name to your new access key and click "Create"
- Download the JSON file containing both your Access Key ID and Secret Access Key

MinIO needs path style bucket URLs, for this you will need to set `S3_FORCE_PATH_STYLE: true` in your environment variables.

## Azure Storage

Azure Blob Storage is not natively compatible with the S3 API. While Lightdash supports external object storage by allowing integration with S3-compatible APIs, Azure's storage service does not provide this compatibility out of the box. This means that you **cannot use Azure Blob Storage as a drop-in replacement for S3** in Lightdash deployments.

Instead, you can use one of the following S3-compatible solutions within your Azure setup:

- [**MinIO**](https://min.io/)**:** S3-compatible object storage
- [**s3proxy**](https://github.com/gaul/s3proxy)**:** A lightweight proxy that adds an S3-compatible API layer on top of Azure Blob Storage.

## Configure Lightdash to use S3 credentials

Now you have the bucket and the S3 credentials for your cloud storage, you can provide the following ENV variables to Lightdash so it can use the cloud storage using S3 library

```bash
S3_REGION
S3_ACCESS_KEY
S3_SECRET_KEY
S3_BUCKET
S3_ENDPOINT
S3_EXPIRATION_TIME
```

We also support IAM roles. If you don't provide `S3_ACCESS_KEY` and `S3_SECRET_KEY` the S3 library will try to use IAM roles, see this [AWS documentation](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-credentials-node.html) for more details.

The expiration time is optional and defaults to 259200 (seconds).

If you try to customize this expiration time, you should keep in mind that link to your object url will be available for the maximum of 7 days due to signed url limitations if you use IAM role to generate it.